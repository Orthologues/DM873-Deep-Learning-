{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 \"Regularization\" on Oct 5th, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/jiawei/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "Size of train data: 60000, Size of test data: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loads the data\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "# plots a single digit from the data\n",
    "plt.imshow(train_data[0])\n",
    "print(train_data[0].shape)\n",
    "print(f\"Size of train data: {train_data.shape[0]}, Size of test data: {test_data.shape[0]}\")\n",
    "# Reshapes the data to work in an feed forward network (FFN)\n",
    "train_data = train_data.reshape((train_data.shape[0], train_data[0].shape[0]*train_data[0].shape[1]))\n",
    "test_data = test_data.reshape((test_data.shape[0], test_data[0].shape[0]*test_data[0].shape[1]))\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks for the exercise today\n",
    "1. Create an identical model, this time adding some regularization and using the Functional API. Can you increase the accuracy of the model?\n",
    "2. Try adding dropout between some layer, where do you find this has the highest impact?\n",
    "3. How does L1 compare to L2 regularization?\n",
    "4. Vary the number of units, the amount of layers, activation fucntions etc, to obtain the best accuracy you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an identical model, this time adding some regularization and using the Functional API. Can you increase the accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functional API instead of \"Sequential\" model, which is applicable to more cases\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 3.5155 - accuracy: 0.7988\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 1.6014 - accuracy: 0.8923\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.1566 - accuracy: 0.9181\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.8537 - accuracy: 0.9293\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.6706 - accuracy: 0.9325\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.5527 - accuracy: 0.9392\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4673 - accuracy: 0.9432\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4278 - accuracy: 0.9453\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3849 - accuracy: 0.9495\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3649 - accuracy: 0.9521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b882b94d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(train_data[0].shape[0], )) # shape=(784, batch_size)\n",
    "# Add two dense layers to the NN model ($x as a variable representing non-input and non-output layers)\n",
    "# Add a dense layer with 64 output nodes with $inputs as the input layer\n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1(1*1e-3))(inputs) \n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1(1*1e-3))(x)\n",
    "output = Dense(units=10, activation='softmax')(x)\n",
    "# input tensor, output tensor\n",
    "model_l1 = Model(inputs, output) \n",
    "# Compile the L1 model\n",
    "model_l1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1.fit(train_data, train_labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using L2 regularization (regularization rate is ten times of L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 3.6518 - accuracy: 0.8404\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.2996 - accuracy: 0.9170\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.8284 - accuracy: 0.9374\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.5853 - accuracy: 0.9431\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4292 - accuracy: 0.9489\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3425 - accuracy: 0.9513\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.2982 - accuracy: 0.9541\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.2768 - accuracy: 0.9543\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 4s 5ms/step - loss: 0.2582 - accuracy: 0.9563\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.2516 - accuracy: 0.9566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b88400150>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(train_data[0].shape[0], )) # shape=(784, batch_size)\n",
    "# Add two dense layers to the NN model ($x as a variable representing non-input and non-output layers)\n",
    "# Add a dense layer with 64 output nodes with $inputs as the input layer\n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l2(1e-2))(inputs) \n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l2(1e-2))(x)\n",
    "output = Dense(units=10, activation='softmax')(x)\n",
    "# input tensor, output tensor\n",
    "model_l2 = Model(inputs, output) \n",
    "# Compile the L2 model\n",
    "model_l2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l2.fit(train_data, train_labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model using L1_L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 4.8407 - accuracy: 0.8347\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0363 - accuracy: 0.9105\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.2134 - accuracy: 0.9319\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.7544 - accuracy: 0.9399\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.5270 - accuracy: 0.9433\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.4336 - accuracy: 0.9456\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3979 - accuracy: 0.9452\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.3844 - accuracy: 0.9475\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3706 - accuracy: 0.9484\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.3819 - accuracy: 0.9488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b885a0dd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(train_data[0].shape[0], )) # shape=(784, batch_size)\n",
    "# Add two dense layers to the NN model ($x as a variable representing non-input and non-output layers)\n",
    "# Add a dense layer with 64 output nodes with $inputs as the input layer\n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1_l2(1e-3, 1e-2))(inputs) \n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1_l2(1e-3, 1e-2))(x)\n",
    "output = Dense(units=10, activation='softmax')(x)\n",
    "# input tensor, output tensor\n",
    "model_l1_l2 = Model(inputs, output) \n",
    "# Compile the L2 model\n",
    "model_l1_l2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_l1_l2.fit(train_data, train_labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could draw the conclusion that L2 regularization performs the best regarding training accuracy, whereas L1_l2 performs the worst training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding dropout between some layer, where do you find this has the highest impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the L1 regularization model with 80% dropout at the input layer: 95.33%\n"
     ]
    }
   ],
   "source": [
    "# testing a dropout rate at 0.8 for the input layer for the L1 model\n",
    "inputs = Input(shape=(train_data[0].shape[0], )) # shape=(784, batch_size)\n",
    "# Add two dense layers to the NN model ($x as a variable representing non-input and non-output layers)\n",
    "# Add a dense layer with 64 output nodes with $inputs as the input layer\n",
    "x = Dropout(rate=0.8)(inputs)\n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1(1e-3))(inputs) \n",
    "x = Dense(units=64, activation='relu', kernel_regularizer=l1(1e-3))(x)\n",
    "output = Dense(units=10, activation='softmax')(x)\n",
    "# input tensor, output tensor\n",
    "model_dropout = Model(inputs, output) \n",
    "# Compile the L2 model\n",
    "model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_dropout.fit(train_data, train_labels, epochs=10, batch_size=64, verbose=0)\n",
    "acc_l1_dropout80 = round(model_dropout.evaluate(test_data, test_labels, verbose=0)[1]*100, 2)\n",
    "print(f\"Accuracy of the L1 regularization model with 80% dropout at the input layer: {acc_l1_dropout80}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does L1 compare to L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1': 95.47, 'L2': 94.61, 'L3': 94.65}\n",
      "The best model is: L1\n"
     ]
    }
   ],
   "source": [
    "acc_l1 = round(model_l1.evaluate(test_data, test_labels, verbose=0)[1]*100, 2)\n",
    "acc_l2 = round(model_l2.evaluate(test_data, test_labels, verbose=0)[1]*100, 2)\n",
    "acc_l1_l2 = round(model_l1_l2.evaluate(test_data, test_labels, verbose=0)[1]*100, 2)\n",
    "\n",
    "model_acc_dict = {\"L1\": acc_l1, \"L2\": acc_l2, \"L3\": acc_l1_l2}\n",
    "print(model_acc_dict)\n",
    "print(f\"The best model is: {[k for k, v in model_acc_dict.items() if v==max(model_acc_dict.values())][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the number of units, the amount of layers, activation fucntions etc, to obtain the best accuracy you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.32999897003174\n",
      "96.85999751091003\n",
      "96.78000211715698\n",
      "96.5499997138977\n",
      "96.7199981212616\n",
      "96.74000144004822\n",
      "96.79999947547913\n",
      "96.31999731063843\n",
      "96.61999940872192\n",
      "96.03999853134155\n",
      "96.56999707221985\n",
      "96.70000076293945\n",
      "96.78999781608582\n",
      "96.35999798774719\n",
      "96.85999751091003\n",
      "96.42000198364258\n",
      "96.60000205039978\n",
      "96.28000259399414\n",
      "96.64000272750854\n",
      "96.56999707221985\n",
      "96.59000039100647\n",
      "96.34000062942505\n",
      "96.4900016784668\n",
      "96.70000076293945\n",
      "96.5499997138977\n",
      "96.43999934196472\n",
      "96.10999822616577\n",
      "96.25999927520752\n",
      "96.10000252723694\n",
      "96.10000252723694\n",
      "95.74000239372253\n",
      "95.660001039505\n",
      "96.20000123977661\n",
      "95.16000151634216\n",
      "95.99000215530396\n",
      "95.8400011062622\n",
      "95.59000134468079\n",
      "96.29999995231628\n",
      "94.9500024318695\n",
      "95.80000042915344\n",
      "95.49000263214111\n",
      "95.57999968528748\n",
      "95.27999758720398\n",
      "95.20000219345093\n",
      "95.09000182151794\n",
      "95.67999839782715\n",
      "95.49999833106995\n",
      "95.49000263214111\n",
      "Best accuracy: 96.86, Best params: {'dense_layer_units': 16, 'num_dense_layers': 3, 'activation_function': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "candidate_hidden_layer_units: tuple = (16, 32, 64, 128)\n",
    "candidate_num_hidden_layers: range = range(2, 6) # try 2-5 hidden layers\n",
    "candidate_act_funcs: tuple = (\"relu\", \"sigmoid\", \"tanh\")\n",
    "\n",
    "max_acc: float = 0\n",
    "best_params: dict = {\"dense_layer_units\": None, \"num_dense_layers\": None, \"activation_function\": None}\n",
    "\n",
    "# set all candidate neural networks to have L1 regularization\n",
    "for unit in candidate_hidden_layer_units:\n",
    "    for act_func in candidate_act_funcs:\n",
    "        for num_layers in candidate_num_hidden_layers:\n",
    "            inputs = Input(shape=(train_data[0].shape[0], )) # shape=(784, batch_size)\n",
    "            x = Dense(units=unit, activation=act_func, kernel_regularizer=l1(1e-3))(inputs)\n",
    "            hidden_layer_count: int = 1\n",
    "            while hidden_layer_count < num_layers:\n",
    "                x = Dense(units=unit, activation=act_func, kernel_regularizer=l1(1e-3))(x)\n",
    "                hidden_layer_count+=1\n",
    "            output = Dense(units=10, activation='softmax')(x)\n",
    "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(train_data, train_labels, epochs=10, batch_size=64, verbose=0)\n",
    "            acc = model.evaluate(test_data, test_labels, verbose=0)[1]\n",
    "            print(acc*100)\n",
    "            if acc > max_acc: \n",
    "                max_acc = acc\n",
    "                best_params[\"dense_layer_units\"] = unit\n",
    "                best_params[\"num_dense_layers\"] = num_layers\n",
    "                best_params[\"activation_function\"] = act_func\n",
    "\n",
    "print(f\"Best accuracy: {round(max_acc, 4)*100}, Best params: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
